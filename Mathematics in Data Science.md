# 数据科学中的数学 by沈佐伟 Zuowei Shen

数据 $\rhd$ 深度学习 $\rhd$ AI

## What is data science?
<p>
Data Science is science<br>
science for data<br>
data for science<br>
> 好的数据收集有助于发现新的科技
<p>

e.g:
### Haar Wavelet: 哈尔小波
science for data
-  信号处理
- 图像压缩

data for science
- 小波助力发现引力波


**数学也要解放思想**

science $\rhd$ technology $\rhd$ industry
industry又反过来作用于science

compressed sensing 压缩感知


## Math. in Data Science
**理论联系实际**
*理论不够，就发展理论*

problems-models-algorithms

> thresholding algorithm
> blind blurring
> segmentation in ultrasound image


***
## deeplearning
1. optimization error;
2. generalization error;
3. approximation error;
4. learned model best model





# emancipate humans from mind-numbing work?
1. 第一次工业革命，让人解放体力劳动，那些人去哪工作？？？
2. 第一次工业革命后，让人没有体力劳动，体质下降，体能下降，故有了体育

类比一下？？


## some  good ideas
1. google search engine: how to evaluate a website? by page links
2. alphago
3. chatgpt

## How to ask?
<p>
(姚大力也如是说，如何问出新的问题，一个核心的问题，让后续继续去解答)<br>
(很多时候会发现过去某个问题突然失去意义，然后再想出新的问题，然后去解决新问题)<br>
数学中也是，提出一个入费马猜想的好问题，让后续研究都围绕它
<p>

## knowledge
1. 知道的知识，和能让你解决问题的知识<br>
2. 要想解决问题，必须整合知识，即便是数学学科内部




***

问答环节
大语言模型 生成模型 非监督是统计问题/supervised是逼近问题<br>
level setter simple funtion,追求simple function


>豆包says

你提的这些概念是理解机器学习模型性能的核心，精准区分它们能帮你快速定位模型欠拟合、过拟合等问题。核心逻辑是：泛化误差是我们的最终目标，它由近似误差和优化误差共同决定，而学习模型则是朝着“最优模型”逼近的产物。


### 1. 三大误差的核心定义
每个误差的本质、影响因素完全不同，是分析模型问题的关键切入点。
- **近似误差（Approximation Error）**：假设空间（所有可选的模型类型，如线性模型、决策树）中“最好的模型”，与“真实数据规律”（理想最优模型）之间的误差。  
  仅由模型**表达能力**决定，和数据量、训练算法无关。比如用线性模型拟合二次函数，近似误差天生就大。
- **优化误差（Optimization Error）**：训练得到的“学习模型”，与“假设空间内的最优模型”之间的误差。  
  由**优化算法性能**决定，比如梯度下降是否收敛到全局最优、迭代次数是否足够。比如训练时没收敛，优化误差就大。
- **泛化误差（Generalization Error）**：“学习模型”在未见过的测试数据上的误差，是评估模型好坏的最终标准。  
  核心构成：泛化误差 ≈ 近似误差 + 优化误差（数据量足够大时，统计误差可忽略）。比如泛化差，可能是模型太简单（近似误差大），也可能是没训好（优化误差大）。


### 2. 学习模型 vs 最优模型
需要区分“理想最优”和“现实最优”，避免混淆目标。
- **真实最优模型（True Best Model）**：生成数据的真实规律（如物理公式、用户行为逻辑），是永远无法完全获取的“理想目标”。
- **假设空间内的最优模型**：在选定的模型类型（假设空间）中，理论上能最小化近似误差的模型，是训练的“阶段性目标”。
- **学习模型（Learned Model）**：通过数据和算法实际得到的模型，是对“假设空间内最优模型”的逼近结果，性能用泛化误差衡量。


### 3. 核心关系总结
用逻辑链可清晰梳理它们的关联：
1. 最终目标：最小化学习模型的**泛化误差**。
2. 两大优化方向：
   - 减小近似误差：换用表达能力更强的模型（如从线性模型换成神经网络）。
   - 减小优化误差：调优算法（如选合适的学习率、增加迭代次数），确保模型训到收敛。
3. 关键矛盾：模型表达能力过强（近似误差小）可能导致过拟合，需用正则化等方式平衡泛化性能。


要不要我帮你整理一份**机器学习误差与模型概念对比表**？表格会包含每个术语的定义、影响因素、优化方法，以及核心关系，方便你后续学习或工作中快速查阅。

Learned Model（学习模型）：指用数据和训练算法（如梯度下降、随机森林训练流程）实际 “学” 出来的模型。
它是可落地、可使用的具体模型，比如你用 Python 训练后保存的.pth文件或 sklearn 模型对象，性能用泛化误差（测试集误差）衡量。
Best Model（最优模型）：需分两层理解，避免混淆：
假设空间内的最优模型：在你选定的模型类型（如线性回归、CNN）所构成的 “假设空间” 中，理论上能最小化 “近似误差”（与真实数据规律的差距）的模型，是训练的 “直接目标”。
真实最优模型：指生成数据的 “真实规律”（如用户点击行为的真实逻辑、物理世界的客观公式），是所有模型的 “终极理想目标”，但永远无法完全获取。
数学逼近论是机器学习中“用模型近似真实规律”的**核心数学基础**，它从理论上回答了“为什么能逼近”“用什么逼近”“逼近得有多好”这三个关键问题，直接支撑着学习模型对最优模型（真实数据规律）的逼近过程。


### 1. 核心衔接：逼近论与机器学习的底层逻辑一致
机器学习的本质是“用参数化函数（学习模型）逼近未知的真实函数（真实最优模型）”，而这正是数学逼近论的研究范畴——**用结构简单、易于计算的“逼近函数类”，去近似结构复杂、甚至未知的“目标函数”**。

两者的对应关系非常直接：
- 逼近论中的“目标函数” → 机器学习中的“真实最优模型”（数据背后的真实规律）。
- 逼近论中的“逼近函数类” → 机器学习中的“假设空间”（所有可选的模型类型，如线性函数、神经网络、决策树）。
- 逼近论中的“逼近误差” → 机器学习中的“近似误差”（假设空间内最优模型与真实最优模型的差距）。


### 2. 逼近论的三大核心应用场景
#### （1）为“假设空间选择”提供理论依据：确定“能用什么模型逼近”
逼近论中的经典定理，直接决定了机器学习中“哪些模型类型有能力逼近真实规律”，避免选择“先天不足”的模型（近似误差无法减小）。
- **魏尔斯特拉斯逼近定理**：闭区间上的连续函数，可被多项式函数“一致逼近”（误差任意小）。  
  → 对应机器学习：多项式回归、线性模型（一次多项式）有理论基础逼近连续的真实函数，只是高次多项式会因“龙格现象”（边缘震荡）导致过拟合，需结合正则化。
- **Stone-Weierstrass定理**：对更一般的拓扑空间上的函数，若逼近函数类满足“分离点”和“非零常数”条件，就能一致逼近目标函数。  
  → 对应机器学习：解释了为什么神经网络（激活函数满足条件时）、核函数（如RBF核）能逼近高维空间中的复杂非线性函数，这也是“神经网络具有万能逼近性”的理论源头。
- **样条逼近理论**：用分段光滑的低次多项式（样条函数）逼近高维或复杂函数，兼顾精度与稳定性。  
  → 对应机器学习：决策树（分段常数函数）、随机森林、梯度提升树（GBDT）本质是样条逼近的延伸，通过分段拟合降低高维数据的逼近难度，避免单一高次函数的震荡问题。

#### （2）为“误差分析”提供量化工具：回答“逼近得有多好”
逼近论中对“逼近误差阶”（误差随参数变化的速率）的研究，直接支撑机器学习中“近似误差”“泛化误差”的量化分析，指导模型调优。
- **逼近误差阶的估计**：比如用n个节点的多项式逼近光滑函数，误差阶为O(1/n^k)（k是函数光滑度）；用神经网络逼近，误差阶与网络宽度、深度的关系。  
  → 对应机器学习：可通过逼近论推导“模型复杂度（如多项式次数、网络深度）与近似误差的关系”，比如“当真实函数光滑时，增加神经网络深度能更快降低近似误差”，为模型结构设计提供依据。
- **Lp逼近与损失函数**：逼近论中常用Lp范数（如L1、L2）衡量逼近误差，这直接对应机器学习中的损失函数（L1损失、MSE损失）。  
  → 例如，L2逼近（最小二乘）对应MSE损失，理论上在高斯噪声下是最优估计；L1逼近对应MAE损失，对异常值更鲁棒，这些都是逼近论中“最优逼近”理论的直接应用。

#### （3）为“算法设计”提供思想支撑：优化“逼近过程”
逼近论中的迭代逼近、自适应逼近等方法，影响了机器学习中模型训练算法的设计，尤其是如何高效缩小“学习模型与最优模型”的差距（即优化误差）。
- **迭代逼近（如梯度下降的思想源头）**：逼近论中通过“逐次修正”逼近目标函数，比如牛顿迭代、共轭梯度法，这与机器学习中“梯度下降通过迭代更新参数，逐步降低损失”的逻辑完全一致。  
  → 逼近论中对迭代收敛速率的分析（如线性收敛、超线性收敛），直接用于证明梯度下降、Adam等优化算法的收敛性，指导学习率、动量等超参数的设置。
- **自适应逼近**：根据目标函数的局部特征（如光滑度、变化率）调整逼近策略，比如在函数变化剧烈的区域用更密的节点。  
  → 对应机器学习：自适应学习率算法（如AdaGrad、RMSprop）、决策树的节点分裂策略（在信息增益大的特征上优先分裂，即函数变化剧烈的“区域”），本质都是自适应逼近思想的体现。


### 3. 关键价值：避免“盲目调参”，提供理论保障
没有逼近论，机器学习的模型选择和调优可能陷入“经验主义”。而逼近论的作用在于：
- 明确边界：比如“线性模型无法逼近非线性真实函数”（除非真实函数本身线性），避免在不适合的模型上浪费精力。
- 量化指导：比如“要将近似误差降低到某个阈值，至少需要多少模型复杂度（如网络深度）”，为模型设计提供量化目标。
- 解释现象：比如“为什么增加数据量能降低泛化误差”，可通过逼近论中“样本数与逼近误差的关系”解释（数据越多，对目标函数的采样越充分，逼近越准确）。


要不要我帮你整理一份**“数学逼近论核心定理与机器学习应用对应表”**？表格会列出魏尔斯特拉斯定理、Stone-Weierstrass定理等关键定理，以及它们在模型选择、误差分析、算法设计中的具体应用场景，方便你快速查阅和关联知识点。





